{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "925a6625-e941-4edf-ad8f-aa8b8b8103ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9ad261-29bd-4431-acb3-53e965339c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'from pyspark.sql import SparkSession', 'print(locals())'], '_oh': {}, '_dh': [WindowsPath('C:/Users/Windows 10/Desktop/MyData/pyspark')], 'In': ['', 'from pyspark.sql import SparkSession', 'print(locals())'], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x000001B44D6BC590>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x000001B44E6A86D0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x000001B44E6A86D0>, 'open': <function open at 0x000001B44C7E7A60>, '_': '', '__': '', '___': '', '__session__': 'C:\\\\Users\\\\Windows 10\\\\Desktop\\\\MyData\\\\pyspark\\\\pyspark_jupyter.ipynb', '_i': 'from pyspark.sql import SparkSession', '_ii': '', '_iii': '', '_i1': 'from pyspark.sql import SparkSession', 'SparkSession': <class 'pyspark.sql.session.SparkSession'>, '_i2': 'print(locals())'}\n"
     ]
    }
   ],
   "source": [
    "print(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10881906-4a97-4779-86a1-953a794a073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Datacamp Pyspark Tutorial\").config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "352e199d-d598-4ecf-9c73-caedceecfa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/10 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/10 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/10 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/10 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/10 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/10 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/10 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/10 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/10 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/10 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/10 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/10 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/10 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/10 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/10 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/10 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/10 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/10 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/10 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/10 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+\n",
      "|Quantity|\n",
      "+--------+\n",
      "|       6|\n",
      "|       6|\n",
      "|       8|\n",
      "|       6|\n",
      "|       6|\n",
      "|       2|\n",
      "|       6|\n",
      "|       6|\n",
      "|       6|\n",
      "|      32|\n",
      "|       6|\n",
      "|       6|\n",
      "|       8|\n",
      "|       6|\n",
      "|       6|\n",
      "|       3|\n",
      "|       2|\n",
      "|       3|\n",
      "|       3|\n",
      "|       4|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|StockCode|         Description|\n",
      "+---------+--------------------+\n",
      "|   85123A|WHITE HANGING HEA...|\n",
      "|    71053| WHITE METAL LANTERN|\n",
      "|   84406B|CREAM CUPID HEART...|\n",
      "|   84029G|KNITTED UNION FLA...|\n",
      "|   84029E|RED WOOLLY HOTTIE...|\n",
      "|    22752|SET 7 BABUSHKA NE...|\n",
      "|    21730|GLASS STAR FROSTE...|\n",
      "|    22633|HAND WARMER UNION...|\n",
      "|    22632|HAND WARMER RED P...|\n",
      "|    84879|ASSORTED COLOUR B...|\n",
      "|    22745|POPPY'S PLAYHOUSE...|\n",
      "|    22748|POPPY'S PLAYHOUSE...|\n",
      "|    22749|FELTCRAFT PRINCES...|\n",
      "|    22310|IVORY KNITTED MUG...|\n",
      "|    84969|BOX OF 6 ASSORTED...|\n",
      "|    22623|BOX OF VINTAGE JI...|\n",
      "|    22622|BOX OF VINTAGE AL...|\n",
      "|    21754|HOME BUILDING BLO...|\n",
      "|    21755|LOVE BUILDING BLO...|\n",
      "|    21777|RECIPE BOX WITH M...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+------------------+------------------+--------------------+------------------+-------------+-----------------+------------------+-----------+\n",
      "|summary|         InvoiceNo|         StockCode|         Description|          Quantity|  InvoiceDate|        UnitPrice|        CustomerID|    Country|\n",
      "+-------+------------------+------------------+--------------------+------------------+-------------+-----------------+------------------+-----------+\n",
      "|  count|            541909|            541909|              540455|            541909|       541909|           541909|            406829|     541909|\n",
      "|   mean|  559965.752026781|27623.240210938104|             20713.0|  9.55224954743324|         NULL|4.611113626089702|15287.690570239585|       NULL|\n",
      "| stddev|13428.417280797923|16799.737628427676|                NULL|218.08115785023432|         NULL|96.75985306117958|1713.6003033215952|       NULL|\n",
      "|    min|            536365|             10002| 4 PURPLE FLOCK D...|                -1|1/10/11 10:04|        -11062.06|             12346|  Australia|\n",
      "|    max|           C581569|                 m|   wrongly sold sets|               992|  9/9/11 9:52|            99.96|             18287|Unspecified|\n",
      "+-------+------------------+------------------+--------------------+------------------+-------------+-----------------+------------------+-----------+\n",
      "\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|unitprice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/10 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/10 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/10 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/10 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/10 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/10 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/10 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/10 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/10 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/10 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "\n",
      "+----------+---------+\n",
      "|CustomerID|UnitPrice|\n",
      "+----------+---------+\n",
      "|     17850|     2.55|\n",
      "|     17850|     3.39|\n",
      "|     17850|     2.75|\n",
      "|     17850|     3.39|\n",
      "|     17850|     3.39|\n",
      "|     17850|     7.65|\n",
      "|     17850|     4.25|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+---------+\n",
      "|CustomerID|UnitPrice|\n",
      "+----------+---------+\n",
      "|     17850|     2.55|\n",
      "|     17850|     3.39|\n",
      "|     17850|     2.75|\n",
      "|     17850|     3.39|\n",
      "|     17850|     3.39|\n",
      "|     17850|     7.65|\n",
      "|     17850|     4.25|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+---------+\n",
      "|CustomerID|UnitPrice|\n",
      "+----------+---------+\n",
      "|     17850|     3.39|\n",
      "|     17850|     3.39|\n",
      "|     17850|     3.39|\n",
      "|     17850|     7.65|\n",
      "|     17850|     4.25|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+---------+\n",
      "|CustomerID|UnitPrice|\n",
      "+----------+---------+\n",
      "|     17850|     2.55|\n",
      "|     17850|     2.75|\n",
      "+----------+---------+\n",
      "\n",
      "+---------+\n",
      "|StockCode|\n",
      "+---------+\n",
      "|    22728|\n",
      "|    21889|\n",
      "|   90210B|\n",
      "|    21259|\n",
      "|    21894|\n",
      "|    21452|\n",
      "|    22121|\n",
      "|    90022|\n",
      "|    21249|\n",
      "|    90143|\n",
      "|    84881|\n",
      "|    21248|\n",
      "|    22254|\n",
      "|    20868|\n",
      "|    21331|\n",
      "|   90197B|\n",
      "|    22596|\n",
      "|    21711|\n",
      "|    22130|\n",
      "|    22314|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|  InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------+---------+----------+--------------+\n",
      "|   536386|   85099C|JUMBO  BAG BAROQU...|     100| 12/1/10 9:57|     1.65|     16029|United Kingdom|\n",
      "|   536386|   85099B|JUMBO BAG RED RET...|     100| 12/1/10 9:57|     1.65|     16029|United Kingdom|\n",
      "|   536390|    22197|SMALL POPCORN HOLDER|     100|12/1/10 10:19|     0.72|     17511|United Kingdom|\n",
      "|   536390|   85099B|JUMBO BAG RED RET...|     100|12/1/10 10:19|     1.65|     17511|United Kingdom|\n",
      "|   536783|    20719|WOODLAND CHARLOTT...|     100|12/2/10 15:19|     0.72|     15061|United Kingdom|\n",
      "|   536785|    20963|   APPLE BATH SPONGE|     100|12/2/10 15:22|     1.25|     15061|United Kingdom|\n",
      "|   536785|    20961|STRAWBERRY BATH S...|     100|12/2/10 15:22|     1.25|     15061|United Kingdom|\n",
      "|   536831|    21929|JUMBO BAG PINK VI...|     100|12/2/10 17:40|     1.65|     16029|United Kingdom|\n",
      "|   536831|    21928|JUMBO BAG SCANDIN...|     100|12/2/10 17:40|     1.65|     16029|United Kingdom|\n",
      "|   536848|    21930|JUMBO STORAGE BAG...|     100| 12/3/10 9:39|     1.65|     13408|United Kingdom|\n",
      "|   536848|    21928|JUMBO BAG SCANDIN...|     100| 12/3/10 9:39|     1.65|     13408|United Kingdom|\n",
      "|   536944|    22384|LUNCH BAG PINK PO...|     100|12/3/10 12:20|     1.45|     12557|         Spain|\n",
      "|   536944|    20728| LUNCH BAG CARS BLUE|     100|12/3/10 12:20|     1.45|     12557|         Spain|\n",
      "|   536954|    22943|CHRISTMAS LIGHTS ...|     100|12/3/10 12:41|     4.25|     17809|United Kingdom|\n",
      "|   537195|    22355|CHARLOTTE BAG SUK...|     100|12/5/10 13:55|     0.72|     15311|United Kingdom|\n",
      "|   537227|    22383|LUNCH BAG SUKI  D...|     100| 12/6/10 8:42|     1.45|     17677|United Kingdom|\n",
      "|   537227|   85099B|JUMBO BAG RED RET...|     100| 12/6/10 8:42|     1.65|     17677|United Kingdom|\n",
      "|   537262|    22197|SMALL POPCORN HOLDER|     100|12/6/10 11:26|     0.72|     15039|United Kingdom|\n",
      "|   537476|    22381|TOY TIDY PINK POL...|     100|12/7/10 11:26|     1.85|     15838|United Kingdom|\n",
      "|   537618|    17038|PORCELAIN BUDAH I...|     100|12/7/10 13:50|     0.07|     15061|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+\n",
      "|    sum(UnitPrice)|\n",
      "+------------------+\n",
      "|2498803.9740006444|\n",
      "+------------------+\n",
      "\n",
      "+------------------+---------------+\n",
      "|    sum(UnitPrice)|count(Quantity)|\n",
      "+------------------+---------------+\n",
      "|2498803.9740006444|         541909|\n",
      "+------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sum(Quantity)` cannot be resolved. Did you mean one of the following? [`StockCode`].;\n'Project [StockCode#1507, 'sum(Quantity)]\n+- Aggregate [StockCode#1507], [StockCode#1507]\n   +- Project [InvoiceNo#1506, StockCode#1507, Description#1508, Quantity#1509, InvoiceDate#1510, UnitPrice#1511, CustomerID#1512, Country#1513]\n      +- Project [InvoiceNo#1506, StockCode#1507, Description#1508, Quantity#1509, InvoiceDate#1510, UnitPrice#1511, CustomerID#1512, Country#1513, (cast(UnitPrice#1511 as double) + cast(2 as double)) AS UnitPrice_after_change#2243]\n         +- Relation [InvoiceNo#1506,StockCode#1507,Description#1508,Quantity#1509,InvoiceDate#1510,UnitPrice#1511,CustomerID#1512,Country#1513] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 72\u001b[0m\n\u001b[0;32m     68\u001b[0m df\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnitPrice\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     70\u001b[0m df\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnitPrice\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 72\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStockCode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStockCode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum(Quantity)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3180\u001b[0m \n\u001b[0;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sum(Quantity)` cannot be resolved. Did you mean one of the following? [`StockCode`].;\n'Project [StockCode#1507, 'sum(Quantity)]\n+- Aggregate [StockCode#1507], [StockCode#1507]\n   +- Project [InvoiceNo#1506, StockCode#1507, Description#1508, Quantity#1509, InvoiceDate#1510, UnitPrice#1511, CustomerID#1512, Country#1513]\n      +- Project [InvoiceNo#1506, StockCode#1507, Description#1508, Quantity#1509, InvoiceDate#1510, UnitPrice#1511, CustomerID#1512, Country#1513, (cast(UnitPrice#1511 as double) + cast(2 as double)) AS UnitPrice_after_change#2243]\n         +- Relation [InvoiceNo#1506,StockCode#1507,Description#1508,Quantity#1509,InvoiceDate#1510,UnitPrice#1511,CustomerID#1512,Country#1513] csv\n"
     ]
    }
   ],
   "source": [
    "#reading the data in multiple ways\n",
    "df = spark.read.csv('datacamp_ecommerce.csv',header=True,escape=\"\\\"\")\n",
    "df1= spark.read.option('header','true').csv('datacamp_ecommerce.csv') #inferSchema=true means it will consider integeres otherwise all are strings\n",
    "df.show() #show the data\n",
    "type(df) #type of data\n",
    "\n",
    "# checking the schema of dataframe i.e rows and and their types\n",
    "df.printSchema()\n",
    "df.columns #for getting all columns\n",
    "df.head(3) #selecting the first 3 rows\n",
    "df.tail(3) #selecing the last 3 rows\n",
    "df.show() #selecting all rows\n",
    "df.select('Quantity').show() #selecting single column\n",
    "df.select(['StockCode','Description']).show() #selecting multiple columns\n",
    "df.dtypes #to dsiplay the column types\n",
    "df.describe().show() #summary about dataframe\n",
    "\n",
    "#adding new column in dataframe\n",
    "df=df.withColumn('UnitPrice_after_change',df['UnitPrice']+2)\n",
    "df.withColumn('UnitPrice',df['UnitPrice']+2)#updating the existing column values\n",
    "\n",
    "#removing the columns\n",
    "df =df.drop('UnitPrice_after_change')\n",
    "\n",
    "#renaming the column name\n",
    "df.withColumnRenamed('UnitPrice','unitprice').show()\n",
    "\n",
    "#dropping the rows which are contains atleast one null  in the row\n",
    "df.na.drop().count()\n",
    "\n",
    "#how = \"any\"  how=\"all\"\n",
    "df.na.drop(how=\"any\").count()\n",
    "\n",
    "# how=\"all\"\n",
    "df.na.drop(how=\"all\").count()\n",
    "\n",
    "# thresh #drop rows does not contain thresold non-null values in row\n",
    "df.na.drop(thresh=8).count()\n",
    "\n",
    "# thresh\n",
    "df.na.drop(thresh=10).count() #returns 0 because we have only 8 columns\n",
    "\n",
    "#subset #droping the rows which contain null values in particular columns\n",
    "df.na.drop(subset=['UnitPrice','Description']).count()\n",
    "\n",
    "#### Filter operations\n",
    "# selecting the rows based on condition\n",
    "df.filter(\"InvoiceNo=536365\").show()\n",
    "\n",
    "# selecting the columns after applying the condition\n",
    "df.filter(\"InvoiceNo=536365\").select(['CustomerID','UnitPrice']).show()\n",
    "\n",
    "df.filter(df[\"InvoiceNo\"]==536365).select(['CustomerID','UnitPrice']).show()\n",
    "\n",
    "# applying multiple conditions using   &  |  ~\n",
    "\n",
    "df.filter( (df[\"InvoiceNo\"]==536365)\n",
    "          & (df[\"UnitPrice\"]>=3) ).select(['CustomerID','UnitPrice']).show()\n",
    "\n",
    "df.filter( (df[\"InvoiceNo\"]==536365)\n",
    "          & ~(df[\"UnitPrice\"]>=3) ).select(['CustomerID','UnitPrice']).show()\n",
    "\n",
    "##### groupby and aggregate functions\n",
    "df.groupBy('StockCode').sum().show()\n",
    "\n",
    "df.filter(df['Quantity']==100).show()\n",
    "\n",
    "df.agg({'UnitPrice':'sum'}).show()\n",
    "\n",
    "df.agg({'UnitPrice':'sum','Quantity':'count'}).show()\n",
    "\n",
    "df.groupBy('StockCode').sum().select(['StockCode','sum(Quantity)']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a73ecd-3b47-4740-bf53-ac9f7f26f9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|   b|\n",
      "+---+----+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe manually\n",
    "from pyspark.sql.types import *\n",
    "# Method 1  --> using list of tuples\n",
    "df=spark.createDataFrame(data=[(1,1),(2,2),(3,3),(4,4)],schema=['id','data'])\n",
    "# df.show()\n",
    "\n",
    "# Method 2 --> using list of dictionaries\n",
    "data=[{'id':1,'name':'a'},{'id':2,'name':'b'}]\n",
    "df1=spark.createDataFrame(data=data)\n",
    "df1.show()\n",
    "\n",
    "# Method 3 -- by proving the schema for the dtaframe\n",
    "# provinding the schema details for dataframe i.e like creating the table\n",
    "schema=StructType([StructField(name=\"id\",dataType=IntegerType()),\n",
    "                   StructField(name=\"data\",dataType=StringType()) ])\n",
    "                   \n",
    "schema1=StructType().add(field=\"id\",data_type=IntegerType()).add(field=\"data\",data_type=StringType())\n",
    "schema2=StructType().add(\"id\",IntegerType(),True).add(\"data\",StringType(),True)\n",
    "\n",
    "data=[(1,'a'),(2,'b'),(3,'c'),(4,'d')]#inserting data in table \n",
    "df2=spark.createDataFrame(data=data,schema=schema2) #creating the dataframe\n",
    "# df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e996be50-398e-43aa-be19-a8d0d92d5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+----------+----------+------+-------+---------+--------------------+--------------------+--------------------+--------------------+--------------+--------------+--------------+\n",
      "|Series_reference| Period|Data_value|Suppressed|STATUS|  UNITS|Magnitude|             Subject|               Group|      Series_title_1|      Series_title_2|Series_title_3|Series_title_4|Series_title_5|\n",
      "+----------------+-------+----------+----------+------+-------+---------+--------------------+--------------------+--------------------+--------------------+--------------+--------------+--------------+\n",
      "|   BDCQ.SF1AA2CA|2016.06|  1116.386|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2016.09|  1070.874|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2016.12|  1054.408|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2017.03|  1010.665|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2017.06|    1233.7|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2017.09|  1282.436|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2017.12|   1290.82|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2018.03|  1412.007|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2018.06|  1488.055|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2018.09|  1497.678|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2018.12|  1570.507|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2019.03|  1393.749|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2019.06|  1517.143|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2019.09|  1381.514|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2019.12|  1370.985|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2020.03|  1073.017|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2020.06|  1131.445|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2020.09|  1440.101|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2020.12|  1489.979|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "|   BDCQ.SF1AA2CA|2021.03|  1390.782|      NULL|     F|Dollars|        6|Business Data Col...|Industry by finan...|Sales (operating ...|Forestry and Logging|Current prices|    Unadjusted|          NULL|\n",
      "+----------------+-------+----------+----------+------+-------+---------+--------------------+--------------------+--------------------+--------------------+--------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########## read csv files \n",
    "# Method 1\n",
    "csv_data = spark.read.csv('csv_files/csv_data.csv',header=True,inferSchema=True)\n",
    "# display(csv_data)  this line works in databricks for displaying the data\n",
    "csv_data.show()\n",
    "# csv_data.printSchema()\n",
    "\n",
    "# Method 2\n",
    "# csv_data1=spark.read.format(\"csv\").load(\"/FileStore/tables/csv_data1.csv\",header=True)\n",
    "# csv_data1.show()\n",
    "# display(csv_data1)\n",
    "\n",
    "# Method 3\n",
    "# csv_data2=spark.read.format(\"csv\").option(key='header',value='True').load(\"/FileStore/tables/csv_data2.csv\",inferSchema=True)\n",
    "# display(csv_data2)\n",
    "\n",
    "# Method 4 --> read multiple csv files from different folders\n",
    "# csv_data3=spark.read.csv(path=[\"/FileStore/tables/csv_data1.csv\",\"/FileStore/tables/csv_data2.csv\"],header=True)\n",
    "# display(csv_data3)\n",
    "# csv_data3.printSchema()\n",
    "\n",
    "# Method 5 --> read multiple csv files from same path\n",
    "# csv_data4=spark.read.csv(path=\"/FileStore/tables/\",header=True)\n",
    "# display(csv_data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96771773-45ad-429a-a496-901abefc02e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|   b|\n",
      "|  3|   c|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######### saving the dataframe into csv file with different ways\n",
    "from pyspark.sql import *\n",
    "df=spark.createDataFrame([(1,'a'),(2,'b'),(3,'c')],schema=[\"id\",\"data\"])\n",
    "df.show()\n",
    "#######  not working in my system due to java issue\n",
    "# df.write.csv(\"FileStore/csv_files/abc.csv\",header=True,mode=\"ignore\") #adding with modes\n",
    "# df.write.option(\"header\",True).csv(\"/FileStore/tables/abc/abc.csv\") #stores in parts\n",
    "# df.write.mode(\"ignore\").csv(\"/FileStore/csv_files/abc.csv\",header=True)\n",
    "# df.write.format(\"csv\").mode(\"overwrite\").save(\"/FileStore/csv_files/abc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2eb2b4-74d8-4f2b-8cd1-03003c753621",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### reading json file in different ways\n",
    "df=spark.read.json(\"FileStore/json_data.json\")\n",
    "df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# df1=spark.read.format(\"json\").load(\"/FileStore/json_data.json\")\n",
    "# df1.show()\n",
    "\n",
    "# read multi line json data\n",
    "# df2=spark.read.json(\"/FileStore/multi_line_json_data.json\",multiLine=True)\n",
    "# df2.show()\n",
    "\n",
    "# reading multiple json files\n",
    "# df3=spark.read.json(path=[\"/FileStore/json_data.json\",\"/FileStore/json_data1.json\"])\n",
    "# df3.show()\n",
    "\n",
    "# df4=spark.read.json(path=[\"/FileStore/json_data.json\",\"/FileStore/multi_line_json_data.json\"],multiLine=True)\n",
    "# df4.show()\n",
    "\n",
    "# reading all json files from directory\n",
    "# df5=spark.read.json(path=\"FileStore/*.json\",multiLine=True)\n",
    "# df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f487ca69-e8db-4f09-8c94-3b4ac704922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1| jkl|\n",
      "|  2| abc|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#########  write dataframe into json file\n",
    "# df=spark.createDataFrame([{\"id\":1,\"name\":\"raj\"},{\"id\":2,\"name\":\"ram\"}])\n",
    "# df.show()\n",
    "\n",
    "schema=[\"id\",\"name\"]\n",
    "df1=spark.createDataFrame([(1,\"jkl\"),(2,\"abc\")],schema=schema)\n",
    "df1.show()\n",
    "\n",
    "# df.write.json(path=\"FileStore/output.json\",mode=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1584120f-0309-42cb-8018-26587b71a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|              model| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
      "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
      "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
      "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
      "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
      "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
      "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|\n",
      "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|\n",
      "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n",
      "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n",
      "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|\n",
      "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|\n",
      "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|\n",
      "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|\n",
      "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|\n",
      "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|\n",
      "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|\n",
      "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|\n",
      "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n",
      "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n",
      "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|\n",
      "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ########reading the parquet files\n",
    "df= spark.read.parquet(\"FileStore/cars.parquet\")\n",
    "df.show()\n",
    "\n",
    "# not working\n",
    "# reading multiple files\n",
    "# df1 = spark.read.parquet(path=[\"FileStore/cars.parquet\",\"FileStore/cars1.parquet\"])\n",
    "# df1 = spark.read.parquet(path=[\"FileStore/*.parquet\"])\n",
    "# df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c875a340-9eea-4128-8b44-2e5d7fa8ce9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1| abc|\n",
      "|  2| ghj|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #####saving dataframe into parquet file\n",
    "schema=[\"id\",\"name\"]\n",
    "data=[(1,\"abc\"),(2,\"ghj\")]\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()\n",
    "# df.write.parquet(\"FileStore/output.parquet\",mode=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42b4cfa-58cd-4707-b3c8-700b2d1e982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------\n",
      " id    | 1                    \n",
      " value | asdfgfghfadhtaerh... \n",
      "-RECORD 1---------------------\n",
      " id    | 2                    \n",
      " value | bgrehtsrjhysmjkuk... \n",
      "-RECORD 2---------------------\n",
      " id    | 3                    \n",
      " value | cdsfjghdjhjrahrhl... \n",
      "-RECORD 3---------------------\n",
      " id    | 4                    \n",
      " value | iuywdqbwefhbybygv... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show dataframe contents\n",
    "data=[(1,\"asdfgfghfadhtaerhjtrjyjytskdu\"),\n",
    "      (2,\"bgrehtsrjhysmjkukmukmuykudk\"),\n",
    "      (3,\"cdsfjghdjhjrahrhlekghthty\"),\n",
    "      (4,\"iuywdqbwefhbybygvsvsvd\")]\n",
    "schema=[\"id\",\"value\"]\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "# df.show()\n",
    "# df.show(truncate=False)#to dislpay the full item in rows\n",
    "# df.show(truncate=10) #display specied no. of characters in row element\n",
    "# df.show(n=3)#display the specific no.of rows from dataframe\n",
    "df.show(vertical=True) #show the data in vertical format i.e column names will become row names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17fd9b55-140c-4298-83c7-c121a7533946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|salary|\n",
      "+---+----+------+\n",
      "|  1| ram| 35000|\n",
      "|  2| raj| 48434|\n",
      "|  3|yhak| 59385|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #### withColumn() usage for adding new column or updating the existing column\n",
    "from pyspark.sql.functions import col,lit \n",
    "\n",
    "#col function will return the specified column from dataframe\n",
    "# lit function helps to add data to the column\n",
    "\n",
    "data=[(1,\"ram\",30000),(2,\"raj\",43434),(3,\"yhak\",54385)]\n",
    "schema=[\"id\",\"name\",\"salary\"]\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "# df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# converting the column datatype\n",
    "df1=df.withColumn(colName='salary',col=col('salary').cast('Integer'))\n",
    "# df1.printSchema()\n",
    "\n",
    "# updating the values in column\n",
    "df2=df.withColumn('salary',col('salary')+5000)\n",
    "# df2.show()\n",
    "\n",
    "# add a new column for dataframe\n",
    "# df3=df.withColumn('country',lit('india')) #adding the value to th entire column\n",
    "# df3.show()\n",
    "\n",
    "# duplicating the column in dataframe\n",
    "# df4=df.withColumn('copied_salary',col('salary'))\n",
    "# df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8245170c-f08c-4962-b3bf-4261a42c92b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|salary|\n",
      "+---+----+------+\n",
      "|  1| ram| 30000|\n",
      "|  2| raj| 43434|\n",
      "|  3|yhak| 54385|\n",
      "+---+----+------+\n",
      "\n",
      "+---+----+--------------+\n",
      "| id|name|current_salary|\n",
      "+---+----+--------------+\n",
      "|  1| ram|         30000|\n",
      "|  2| raj|         43434|\n",
      "|  3|yhak|         54385|\n",
      "+---+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #### withColumnRenamed()\n",
    "data=[(1,\"ram\",30000),(2,\"raj\",43434),(3,\"yhak\",54385)]\n",
    "schema=[\"id\",\"name\",\"salary\"]\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# renaming the column name\n",
    "df1=df.withColumnRenamed('salary','current_salary')\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "691babd9-099b-466d-bf08-fd5611384b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------+\n",
      "| id|           name|salary|\n",
      "+---+---------------+------+\n",
      "|  1|   {ram, kumar}|  3000|\n",
      "|  2|   {raj, kumar}|  4500|\n",
      "|  3|{kalyan, kumar}|  4550|\n",
      "+---+---------------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #### StructType() and StructField() \n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "data=[(1,\"ram\",3000),(2,\"raj\",4500)]\n",
    "schema=StructType([StructField(name=\"id\",dataType=IntegerType()),\n",
    "                   StructField(name=\"name\",dataType=StringType()),\n",
    "                   StructField(name=\"salary\",dataType=IntegerType())])\n",
    "\n",
    "# df=spark.createDataFrame(data=data,schema=schema)\n",
    "# df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# complex data i.e nested data like first name and last name\n",
    "data=[(1,(\"ram\",\"kumar\"),3000),(2,(\"raj\",\"kumar\"),4500),(3,(\"kalyan\",\"kumar\"),4550)]\n",
    "structName=StructType([StructField(name=\"first_name\",dataType=StringType()),\\\n",
    "                       StructField(name=\"last_name\",dataType=StringType())])\n",
    "schema=StructType([StructField(name=\"id\",dataType=IntegerType()),\\\n",
    "                   StructField(name=\"name\",dataType=structName),\\\n",
    "                   StructField(name=\"salary\",dataType=IntegerType())])\n",
    "\n",
    "df1=spark.createDataFrame(data=data,schema=schema)\n",
    "df1.show()\n",
    "# display(df)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b555c4-dc70-4df6-93d7-35e9bef08bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "| id|  name|numbers|\n",
      "+---+------+-------+\n",
      "|  1|   ram| [1, 2]|\n",
      "|  2|   raj| [3, 4]|\n",
      "|  3|kalyan| [5, 6]|\n",
      "+---+------+-------+\n",
      "\n",
      "+---+------+-------+-------------+\n",
      "| id|  name|numbers|first_element|\n",
      "+---+------+-------+-------------+\n",
      "|  1|   ram| [1, 2]|            1|\n",
      "|  2|   raj| [3, 4]|            3|\n",
      "|  3|kalyan| [5, 6]|            5|\n",
      "+---+------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #### arrayType column in dataframe\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,ArrayType\n",
    "data=[(1,\"ram\",[1,2]),(2,\"raj\",[3,4]),(3,\"kalyan\",[5,6])]\n",
    "# schema= [\"id\",\"name\",\"numbers\"]\n",
    "schema=StructType([StructField(name=\"id\",dataType=IntegerType()),\\\n",
    "                   StructField(name=\"name\",dataType=StringType()),\n",
    "                   StructField(name=\"numbers\",dataType=ArrayType(IntegerType()))])\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# adding a new column with array elements\n",
    "df1=df.withColumn('first_element',df.numbers[0])\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51af8a18-69a6-4144-933d-a54cdd0a7502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|num1|num2|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "|   3|   4|\n",
      "|   5|   6|\n",
      "+----+----+\n",
      "\n",
      "+----+----+-------+\n",
      "|num1|num2|numbers|\n",
      "+----+----+-------+\n",
      "|   1|   2| [1, 2]|\n",
      "|   3|   4| [3, 4]|\n",
      "|   5|   6| [5, 6]|\n",
      "+----+----+-------+\n",
      "\n",
      "root\n",
      " |-- num1: long (nullable = true)\n",
      " |-- num2: long (nullable = true)\n",
      " |-- numbers: array (nullable = false)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating dataframe with adding values to array\n",
    "from pyspark.sql.functions import col,array\n",
    "data=[(1,2),(3,4),(5,6)]\n",
    "schema=[\"num1\",\"num2\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "df1=df.withColumn(\"numbers\",array(df.num1,df.num2))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c60f1728-1776-41af-b464-4b1c3c723d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+\n",
      "| id|  name|          skills|\n",
      "+---+------+----------------+\n",
      "|  1|   raj|     [aws, java]|\n",
      "|  2|   ram|    [dotnet, js]|\n",
      "|  3|kalyan|[python, golang]|\n",
      "+---+------+----------------+\n",
      "\n",
      "+---+------+----------------+------+\n",
      "| id|  name|          skills| skill|\n",
      "+---+------+----------------+------+\n",
      "|  1|   raj|     [aws, java]|   aws|\n",
      "|  1|   raj|     [aws, java]|  java|\n",
      "|  2|   ram|    [dotnet, js]|dotnet|\n",
      "|  2|   ram|    [dotnet, js]|    js|\n",
      "|  3|kalyan|[python, golang]|python|\n",
      "|  3|kalyan|[python, golang]|golang|\n",
      "+---+------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ###  explore(),split(),array() array_contains()\n",
    "from pyspark.sql.functions import explode\n",
    "data=[(1,\"raj\",[\"aws\",\"java\"]),(2,\"ram\",[\"dotnet\",\"js\"]),(3,\"kalyan\",[\"python\",\"golang\"])]\n",
    "schema=[\"id\",\"name\",\"skills\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# exlpode will explode the array and create rows with element in array\n",
    "df1=df.withColumn('skill',explode(df.skills))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a8bf397-91ea-4eeb-9078-5d232421fa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: string (nullable = true)\n",
      "\n",
      "+---+------+----------------+--------------------+\n",
      "| id|  name|          skills|         skillsArray|\n",
      "+---+------+----------------+--------------------+\n",
      "|  1|   ram|      aws,python|       [aws, python]|\n",
      "|  2|   raj|     java,dotnet|      [java, dotnet]|\n",
      "|  3|kalyan|csharp,js,golang|[csharp, js, golang]|\n",
      "+---+------+----------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: string (nullable = true)\n",
      " |-- skillsArray: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ### split() in dataframe\n",
    "# split function will split the data based on delimiter and returns array\n",
    "from pyspark.sql.functions import split\n",
    "data=[(1,\"ram\",\"aws,python\"),(2,\"raj\",\"java,dotnet\"),(3,\"kalyan\",\"csharp,js,golang\")]\n",
    "schema=[\"id\",\"name\",\"skills\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "# df.show()\n",
    "df.printSchema()\n",
    "df1=df.withColumn(\"skillsArray\",split(df.skills,\",\"))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7583a4eb-b0c8-41c9-a8da-0eaffed90ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+---------+\n",
      "| id|  name|primary|secondary|\n",
      "+---+------+-------+---------+\n",
      "|  1|   ram|   java|   dotnet|\n",
      "|  2|   raj|    aws|   python|\n",
      "|  3|kalyan| golang|   csharp|\n",
      "+---+------+-------+---------+\n",
      "\n",
      "+---+------+-------+---------+----------------+\n",
      "| id|  name|primary|secondary|          skills|\n",
      "+---+------+-------+---------+----------------+\n",
      "|  1|   ram|   java|   dotnet|  [java, dotnet]|\n",
      "|  2|   raj|    aws|   python|   [aws, python]|\n",
      "|  3|kalyan| golang|   csharp|[golang, csharp]|\n",
      "+---+------+-------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array() method usage\n",
    "# array() will helps to combine multiple columns into single column\n",
    "from pyspark.sql.functions import array\n",
    "data=[(1,\"ram\",\"java\",\"dotnet\"),(2,\"raj\",\"aws\",\"python\"),(3,\"kalyan\",\"golang\",\"csharp\")]\n",
    "schema=[\"id\",\"name\",\"primary\",\"secondary\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df1=df.withColumn(\"skills\",array(df.primary,df.secondary))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a15b3ed5-3c92-4b1d-b7c8-be2cb6bcf24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+--------------+\n",
      "|  name|          skills|containspython|\n",
      "+------+----------------+--------------+\n",
      "|   ram|   [aws, python]|          true|\n",
      "|   raj|[golnag, csharp]|         false|\n",
      "|kalyan|[dotent, oracle]|         false|\n",
      "| kumar|              []|         false|\n",
      "+------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ### array_contains() usgae\n",
    "from pyspark.sql.functions import array_contains\n",
    "# this method will check given value is presnt in tha array or not\n",
    "data=[(\"ram\",[\"aws\",\"python\"]),(\"raj\",[\"golnag\",\"csharp\"]),(\"kalyan\",[\"dotent\",\"oracle\"]),(\"kumar\",[])]\n",
    "schema=[\"name\",\"skills\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "# df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "df1=df.withColumn(\"containspython\",array_contains(df.skills,\"python\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f5edd5e-3c91-4fb2-b8b7-9f1295b3f8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+\n",
      "| id|  name|          skills|\n",
      "+---+------+----------------+\n",
      "|  1|   ram|      aws,python|\n",
      "|  2|   raj|     java,dotnet|\n",
      "|  3|kalyan|csharp,js,golang|\n",
      "+---+------+----------------+\n",
      "\n",
      "+---+------+----------------+--------------+\n",
      "| id|  name|          skills|contain_python|\n",
      "+---+------+----------------+--------------+\n",
      "|  1|   ram|      aws,python|          true|\n",
      "|  2|   raj|     java,dotnet|         false|\n",
      "|  3|kalyan|csharp,js,golang|         false|\n",
      "+---+------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,\"ram\",\"aws,python\"),(2,\"raj\",\"java,dotnet\"),(3,\"kalyan\",\"csharp,js,golang\")]\n",
    "schema=[\"id\",\"name\",\"skills\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df1=df.withColumn(\"contain_python\",array_contains(split(\"skills\",\",\"),\"python\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "432d20ff-b9c0-479a-99d4-82720ccaed79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------+----------+\n",
      "|name|properties                    |hair_color|\n",
      "+----+------------------------------+----------+\n",
      "|ram |{eyes -> black, hair -> black}|black     |\n",
      "|raj |{eyes -> blue, hair -> brown} |brown     |\n",
      "+----+------------------------------+----------+\n",
      "\n",
      "+----+------------------------------+----------+\n",
      "|name|properties                    |eyes_color|\n",
      "+----+------------------------------+----------+\n",
      "|ram |{eyes -> black, hair -> black}|black     |\n",
      "|raj |{eyes -> blue, hair -> brown} |blue      |\n",
      "+----+------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## MapType() method usage\n",
    "from pyspark.sql.types import MapType \n",
    "data=[(\"ram\",{\"hair\":\"black\",\"eyes\":\"black\"}),(\"raj\",{\"hair\":\"brown\",\"eyes\":\"blue\"})]\n",
    "# schema=[\"name\",\"properties\"]\n",
    "schema=StructType([StructField(name=\"name\",dataType=StringType()),\\\n",
    "                   StructField(name=\"properties\",dataType=MapType(keyType=StringType(),valueType=StringType()))])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "\n",
    "# display(df)\n",
    "# df.show(truncate=False)\n",
    "# df.printSchema()\n",
    "# accessing the data from MapType item\n",
    "df1=df.withColumn(\"hair_color\",df.properties['hair'])\n",
    "df1.show(truncate=False)\n",
    "df2=df.withColumn(\"eyes_color\",df.properties.getItem('eyes'))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab1f5c9f-4c1a-426e-9d11-8c9561004bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------+\n",
      "|name|properties                    |\n",
      "+----+------------------------------+\n",
      "|ram |{eyes -> black, hair -> black}|\n",
      "|raj |{eyes -> blue, hair -> brown} |\n",
      "+----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode(),map_keys(),map_values() usage on MapType data\n",
    "from pyspark.sql.functions import explode,map_keys,map_values\n",
    "\n",
    "data=[(\"ram\",{\"eyes\":\"black\",\"hair\":\"black\"}),(\"raj\",{\"eyes\":\"blue\",\"hair\":\"brown\"})]\n",
    "schema=[\"name\",\"properties\"]\n",
    "schema=StructType([StructField(\"name\",StringType()),StructField(\"properties\",MapType(StringType(),StringType()))])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show(truncate=False)\n",
    "# explode function will make 2 columns for keys and values\n",
    "# df1=df.select(df.name,df.properties,explode(df.properties))\n",
    "# df1.show(truncate=False)\n",
    "\n",
    "# df2=df.select(df.name,df.properties,map_keys(df.properties))\n",
    "# df2.show(truncate=False)\n",
    "# df3=df.select(df.name,df.properties,map_values(df.properties))\n",
    "# df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c786009-d1d4-4573-91b8-4fea20df6583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ram 3000\n",
      "+------+------+\n",
      "|  name|salary|\n",
      "+------+------+\n",
      "|   ram|  3000|\n",
      "|kalyan|  1234|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Row() class in python\n",
    "from pyspark.sql import Row\n",
    "# Row as tuple\n",
    "row1=Row('ram',2000)\n",
    "# print(row1[0],row1[1])\n",
    "\n",
    "# Row as class object\n",
    "row2=Row(name=\"raj\",salary=5000)\n",
    "# print(row2.name,row2.salary)\n",
    "\n",
    "# create dataFrame using row objects\n",
    "row3=Row(name=\"kalyan\",salary=3000)\n",
    "row4=Row(name=\"tyu\",salary=2200)\n",
    "data=[row3,row4]\n",
    "# df=spark.createDataFrame(data)\n",
    "# df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# create a class using Row() class\n",
    "person=Row('name','salary')\n",
    "p1=person('ram',3000)\n",
    "p2=person('kalyan',1234)\n",
    "print(p1.name,p1.salary)\n",
    "\n",
    "data=[p1,p2]\n",
    "df1=spark.createDataFrame(data)\n",
    "df1.show()\n",
    "\n",
    "# nested data using Row() class\n",
    "# data=[Row(name=\"ram\",prop=Row(hair=\"balck\",eyes=\"blue\")),Row(name=\"kalyan\",prop=Row(hair=\"brown\",eyes=\"black\"))]\n",
    "# df2=spark.createDataFrame(data)\n",
    "\n",
    "# df2.show()\n",
    "# df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c234f8c6-cc4b-464a-adb5-0222d2413dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+--------------+\n",
      "|name|salary|gender|          prop|\n",
      "+----+------+------+--------------+\n",
      "| ram|  2000|  male|{black, black}|\n",
      "|raji|  3000|female| {brown, blue}|\n",
      "+----+------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column() class creation\n",
    "from pyspark.sql.functions import lit,col\n",
    "from pyspark.sql.types import StringType,StructField,MapType,IntegerType\n",
    "column1=lit(\"abcd\")\n",
    "# print(type(column1))\n",
    "# print(column1)\n",
    "\n",
    "data=[(\"ram\",2000,\"male\",(\"black\",\"black\")),(\"raji\",3000,\"female\",(\"brown\",\"blue\"))]\n",
    "# schema=['name','salary','gender']\n",
    "properties=StructType([StructField(\"hair\",StringType()),StructField(\"eyes\",StringType())])\n",
    "schema=StructType([StructField(\"name\",StringType()),\\\n",
    "                   StructField(\"salary\",IntegerType()),\\\n",
    "                   StructField(\"gender\",StringType()),\\\n",
    "                   StructField(\"prop\",properties)])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "\n",
    "# df1=df.withColumn(\"country\",lit(\"abcd\"))\n",
    "# df1.show()\n",
    "\n",
    "# accesing the column values in different ways\n",
    "# df1.select('name').show()\n",
    "# df1.select(df1['name']).show()\n",
    "# df1.select(col('name')).show()\n",
    "\n",
    "# df1.select(df1.prop).show()\n",
    "# df1.select(df1['prop']).show()\n",
    "# df1.select(df1.prop.eyes).show()\n",
    "# df1.select(df1['prop.hair']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "672bf207-f5d6-443e-9851-cbd31196e571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+------+\n",
      "| id|   name|gender|salary|\n",
      "+---+-------+------+------+\n",
      "|  1|    ram|     M| 20000|\n",
      "|  2|kalyani|     F| 30000|\n",
      "|  3|    ram|      | 23300|\n",
      "+---+-------+------+------+\n",
      "\n",
      "+---+-------+--------------+\n",
      "| id|   name|        gender|\n",
      "+---+-------+--------------+\n",
      "|  1|    ram|          Male|\n",
      "|  2|kalyani|        Female|\n",
      "|  3|    ram|Not to specify|\n",
      "+---+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when() and otherwise() usage\n",
    "from pyspark.sql.functions import when\n",
    "data=[(1,\"ram\",\"M\",20000),(2,\"kalyani\",\"F\",30000),(3,\"ram\",\"\",23300)]\n",
    "schema=[\"id\",\"name\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df1=df.select(\"id\",df.name,when(df.gender=='M','Male').\\\n",
    "                            when(df.gender=='F','Female').\\\n",
    "                            otherwise('Not to specify').alias('gender'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c46c2813-6270-43f6-93a0-633b962aeca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|  1|    ram|  2000|\n",
      "|  2|    raj|  3000|\n",
      "|  3|kalyani|  4000|\n",
      "|  4| ramesh|  2344|\n",
      "+---+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|   ram|  2000|\n",
      "|  2|   raj|  3000|\n",
      "|  4|ramesh|  2344|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias(),asc(),desc(), cast() and like() functions usage\n",
    "data=[(1,\"ram\",2000),(2,\"raj\",3000),(3,\"kalyani\",4000),(4,\"ramesh\",2344)]\n",
    "schema= [\"id\",\"name\",\"salary\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# using alias for renaming the column\n",
    "df1=df.select(df.id.alias(\"emp_id\"),df.name.alias(\"emp_name\"),df.salary.alias(\"emp_salary\"))\n",
    "# df1.show()\n",
    "\n",
    "# asc() function to ascending the rows based on particular column\n",
    "# df2=df.sort(df.name.asc())\n",
    "# df2.show()\n",
    "\n",
    "# desc() to sort the rows basd on column values\n",
    "# df3=df.sort(df.name.desc())\n",
    "# df3.show()\n",
    "\n",
    "# cast() to change the datatype of items in column\n",
    "# df4=df.select(df.id,df.name,df.salary.cast('int'))\n",
    "# df4.printSchema()\n",
    "\n",
    "# like() to filter the rows based on some condition applying on column\n",
    "df5=df.filter(df.name.like('r%'))\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26e16257-7fca-46aa-b1ba-a31666f4fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+------+\n",
      "| id|   name|gender|salary|\n",
      "+---+-------+------+------+\n",
      "|  1|    ram|     M|  2000|\n",
      "|  2|    raj|     M|  3000|\n",
      "|  3|kalyani|     F|  4000|\n",
      "|  4| ramesh|     M|  2344|\n",
      "+---+-------+------+------+\n",
      "\n",
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|   ram|     M|  2000|\n",
      "|  2|   raj|     M|  3000|\n",
      "|  4|ramesh|     M|  2344|\n",
      "+---+------+------+------+\n",
      "\n",
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|   ram|     M|  2000|\n",
      "|  2|   raj|     M|  3000|\n",
      "|  4|ramesh|     M|  2344|\n",
      "+---+------+------+------+\n",
      "\n",
      "+---+----+------+------+\n",
      "| id|name|gender|salary|\n",
      "+---+----+------+------+\n",
      "|  1| ram|     M|  2000|\n",
      "+---+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter(), where() functions usage\n",
    "data=[(1,\"ram\",\"M\",2000),(2,\"raj\",\"M\",3000),(3,\"kalyani\",\"F\",4000),(4,\"ramesh\",\"M\",2344)]\n",
    "schema= [\"id\",\"name\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "# filter usage\n",
    "df1=df.filter(df.gender==\"M\")\n",
    "df1.show()\n",
    "\n",
    "df2= df.filter(\"gender == 'M'\")\n",
    "df2.show()\n",
    "\n",
    "# where() usage\n",
    "df3=df.where(df.salary> 3000)\n",
    "# df3.show()\n",
    "\n",
    "df4=df.where(\"name == 'ram'\")\n",
    "# df4.show()\n",
    "\n",
    "# applying multiple conditions\n",
    "df5=df.filter((df.name == \"ram\")&(df.salary==2000))\n",
    "# df5.show()\n",
    "\n",
    "df6=df.where((df.name.like(\"r%\")) & (df.salary==2000))\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92f4ba59-646b-4d9c-bc8e-9df381222667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+------+\n",
      "| id|   name|gender|salary|\n",
      "+---+-------+------+------+\n",
      "|  1|    ram|     M|  2000|\n",
      "|  1|    ram|     M|  2000|\n",
      "|  2|    raj|     M|  3000|\n",
      "|  3|kalyani|     F|  4000|\n",
      "|  4| ramesh|     M|  2344|\n",
      "|  4|  kamal|     M|  2444|\n",
      "+---+-------+------+------+\n",
      "\n",
      "+---+-------+------+------+\n",
      "| id|   name|gender|salary|\n",
      "+---+-------+------+------+\n",
      "|  1|    ram|     M|  2000|\n",
      "|  4| ramesh|     M|  2344|\n",
      "|  4|  kamal|     M|  2444|\n",
      "|  2|    raj|     M|  3000|\n",
      "|  3|kalyani|     F|  4000|\n",
      "+---+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distinct(),dropDuplicate() functions usage\n",
    "data=[(1,\"ram\",\"M\",2000),(1,\"ram\",\"M\",2000),(2,\"raj\",\"M\",3000),(3,\"kalyani\",\"F\",4000),(4,\"ramesh\",\"M\",2344),(4,\"kamal\",\"M\",2444)]\n",
    "schema= [\"id\",\"name\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "# distinct() usage \n",
    "df1=df.distinct() #distinct will work on all columns i.e thw whole row should be duplicated not a single element in a row\n",
    "# df1.show()\n",
    "\n",
    "# dropDuplicates() usage it will work on all columns/particular columns also\n",
    "df2= df.dropDuplicates() #dropping rows which are completely duplicate\n",
    "# df2.show()\n",
    "\n",
    "df3= df.dropDuplicates(['id'])\n",
    "# df3.show()\n",
    "\n",
    "df4= df.dropDuplicates(['salary'])\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15297614-c208-4f38-9d6f-401d114855b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+------+\n",
      "| id|   name|gender|salary|\n",
      "+---+-------+------+------+\n",
      "|  1|    ram|     M|  2000|\n",
      "|  2|    raj|     M|  3000|\n",
      "|  3|kalyani|     F|  4000|\n",
      "|  4|  kamal|     M|  2444|\n",
      "|  1|    ram|     M|  2000|\n",
      "|  4| ramesh|     M|  2344|\n",
      "+---+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# orderBy(), sort() functions usage\n",
    "data=[(1,\"ram\",\"M\",2000),(2,\"raj\",\"M\",3000),(3,\"kalyani\",\"F\",4000),(4,\"kamal\",\"M\",2444),(1,\"ram\",\"M\",2000),(4,\"ramesh\",\"M\",2344)]\n",
    "schema= [\"id\",\"name\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "# sort() usage\n",
    "df1=df.sort(\"salary\")\n",
    "# df1.show()\n",
    "\n",
    "df2= df.sort(df.name.desc())\n",
    "# df2.show()\n",
    "df3=df.sort(df.name,df.salary)\n",
    "# df3.show()\n",
    "\n",
    "# orderBy() usage\n",
    "df4= df.orderBy(df.name,df.salary.desc())\n",
    "# df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122ef88-8d53-4c2a-80ad-a5cdf7cc105d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3680f0d-06e1-45b3-a8a2-10758a41f9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ae0b0-e9fb-4087-a7a0-a67769c2c18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d18226-6f0f-40f8-a2e5-6a904e9ae62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613966e-7e5e-4ab8-b99e-d6e31b83b85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
